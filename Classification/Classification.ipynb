{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":11,"outputs":[{"output_type":"stream","text":"/kaggle/input/bins.csv\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from __future__ import division, print_function, unicode_literals\n\n# Common imports\nimport numpy as np\nimport os\nfrom scipy.io import arff\nimport pandas as pd\n\n# to make this notebook's output stable across runs\nnp.random.seed(42)\n\n# To plot pretty figures\n%matplotlib inline\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nmpl.rc('axes', labelsize=14)\nmpl.rc('xtick', labelsize=12)\nmpl.rc('ytick', labelsize=12)","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\ndf = pd.read_csv(\"../input/bins.csv\")\ndf1 = df.iloc[:,1:8]\ndf1.head()","execution_count":13,"outputs":[{"output_type":"execute_result","execution_count":13,"data":{"text/plain":"   X1 transaction date  X2 house age  X3 distance to the nearest MRT station  \\\n0             0.272926      0.730594                                0.009513   \n1             0.272926      0.445205                                0.043809   \n2             1.000000      0.303653                                0.083315   \n3             0.909389      0.303653                                0.083315   \n4             0.181223      0.114155                                0.056799   \n\n   X4 number of convenience stores  X5 latitude  X6 longitude  \\\n0                              1.0     0.616941      0.719323   \n1                              0.9     0.584949      0.711451   \n2                              0.5     0.671231      0.758896   \n3                              0.5     0.671231      0.758896   \n4                              0.5     0.573194      0.743153   \n\n  Y house price of unit area  \n0                     Medium  \n1                     Medium  \n2                     Medium  \n3            Moderately High  \n4                     Medium  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>X1 transaction date</th>\n      <th>X2 house age</th>\n      <th>X3 distance to the nearest MRT station</th>\n      <th>X4 number of convenience stores</th>\n      <th>X5 latitude</th>\n      <th>X6 longitude</th>\n      <th>Y house price of unit area</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.272926</td>\n      <td>0.730594</td>\n      <td>0.009513</td>\n      <td>1.0</td>\n      <td>0.616941</td>\n      <td>0.719323</td>\n      <td>Medium</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.272926</td>\n      <td>0.445205</td>\n      <td>0.043809</td>\n      <td>0.9</td>\n      <td>0.584949</td>\n      <td>0.711451</td>\n      <td>Medium</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.000000</td>\n      <td>0.303653</td>\n      <td>0.083315</td>\n      <td>0.5</td>\n      <td>0.671231</td>\n      <td>0.758896</td>\n      <td>Medium</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.909389</td>\n      <td>0.303653</td>\n      <td>0.083315</td>\n      <td>0.5</td>\n      <td>0.671231</td>\n      <td>0.758896</td>\n      <td>Moderately High</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.181223</td>\n      <td>0.114155</td>\n      <td>0.056799</td>\n      <td>0.5</td>\n      <td>0.573194</td>\n      <td>0.743153</td>\n      <td>Medium</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#feature selection\ncolumn_names = ['X1 transaction date','X2 house age','X3 distance to the nearest MRT station','X4 number of convenience stores','X5 latitude','X6 longitude','Y house price of unit area']\ndef get_feature_names(X, col = column_names):\n  try:\n    mask = X.get_support() #list of booleans\n  except AttributeError:\n    mask = X.support_  #Boruta has different attributes from scikit-learn API\n  new_features = [] # The list of your K best features\n  for bool, feature in zip(mask, col):\n    if bool:\n      new_features.append(feature)\n  return new_features","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install Boruta","execution_count":15,"outputs":[{"output_type":"stream","text":"Requirement already satisfied: Boruta in /opt/conda/lib/python3.6/site-packages (0.3)\nRequirement already satisfied: scikit-learn>=0.17.1 in /opt/conda/lib/python3.6/site-packages (from Boruta) (0.22.2.post1)\nRequirement already satisfied: scipy>=0.17.0 in /opt/conda/lib/python3.6/site-packages (from Boruta) (1.4.1)\nRequirement already satisfied: numpy>=1.10.4 in /opt/conda/lib/python3.6/site-packages (from Boruta) (1.18.1)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.6/site-packages (from scikit-learn>=0.17.1->Boruta) (0.14.1)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#To divide data into attributes and labels\nX = df1.drop('Y house price of unit area',axis=1).values\ny = df1['Y house price of unit area'].values","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom boruta import BorutaPy\n#Boruta feature selection\nrf = RandomForestClassifier(n_jobs=-1, class_weight='balanced', max_depth=5)\n\nboruta_selector = BorutaPy(rf, n_estimators='auto', random_state=1)\nboruta_selector.fit(X, y)\nboruta_set = boruta_selector.transform(X)\n\nprint(get_feature_names(boruta_selector))","execution_count":17,"outputs":[{"output_type":"stream","text":"['X2 house age', 'X3 distance to the nearest MRT station', 'X5 latitude', 'X6 longitude']\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import SelectFromModel\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import ExtraTreesClassifier\n#L1-based feature selection\nlsvc = LinearSVC(C=0.01, penalty=\"l1\", dual=False, max_iter = 2000).fit(X, y)\nl_model = SelectFromModel(lsvc, prefit=True)\nl1_set = l_model.transform(X)\nprint(get_feature_names(l_model))\n\n#tree-based feature selection\nclf = ExtraTreesClassifier(n_estimators=50)\nclf = clf.fit(X, y)\ntb_model = SelectFromModel(clf, prefit=True)\ntr_set = tb_model.transform(X)\nprint(get_feature_names(tb_model))","execution_count":18,"outputs":[{"output_type":"stream","text":"['X4 number of convenience stores']\n['X2 house age', 'X3 distance to the nearest MRT station', 'X6 longitude']\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\nfrom imblearn.over_sampling import RandomOverSampler\n\n#rebalance the dataset using oversampling (random oversampling)\nros = RandomOverSampler(random_state=42)\n\nros_boruta_set, ros_boruta_labels = ros.fit_resample(boruta_set, y)\nprint(\"Class distribution of oversampling with train_set_boruta \" + str(sorted(Counter(ros_boruta_labels).items())))\n\nros_tr_set, ros_tr_labels = ros.fit_resample(tr_set, y)\nprint(\"Class distribution of oversampling with train_set_tr \" + str(sorted(Counter(ros_tr_labels).items())))","execution_count":19,"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"},{"output_type":"stream","text":"Class distribution of oversampling with train_set_boruta [('High', 226), ('Low', 226), ('Medium', 226), ('Moderately High', 226)]\nClass distribution of oversampling with train_set_tr [('High', 226), ('Low', 226), ('Medium', 226), ('Moderately High', 226)]\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from imblearn.under_sampling import RepeatedEditedNearestNeighbours\n#rebalance the dataset using undersampling (nearest neightbours)\nrenn = RepeatedEditedNearestNeighbours()\n\nrenn_boruta_set, renn_boruta_labels = renn.fit_resample(boruta_set, y)\nprint(\"Class distribution of undersampling with boruta_set \" + str(sorted(Counter(renn_boruta_labels).items())))\n\nrenn_tr_set, renn_tr_labels = renn.fit_resample(tr_set, y)\nprint(\"Class distribution of undersampling with tr_set \" + str(sorted(Counter(renn_tr_labels).items())))","execution_count":20,"outputs":[{"output_type":"stream","text":"Class distribution of undersampling with boruta_set [('High', 4), ('Low', 84), ('Medium', 123), ('Moderately High', 23)]\nClass distribution of undersampling with tr_set [('High', 4), ('Low', 80), ('Medium', 112), ('Moderately High', 23)]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#split the dataset into train and test set\nfrom sklearn.model_selection import train_test_split\n\n#1. split original_set and original_labels\noriginal_set_train, original_set_test, original_labels_train, original_labels_test = train_test_split(X, y, test_size=0.2,random_state=42)\n\n#2. split ros_boruta_set and ros_boruta_labels\nros_boruta_set_train, ros_boruta_set_test, ros_boruta_labels_train, ros_boruta_labels_test = train_test_split(ros_boruta_set,ros_boruta_labels, test_size=0.2, random_state=42)\n\n#3. split ros_tr_set and ros_tr_labels\nros_tr_set_train, ros_tr_set_test, ros_tr_labels_train, ros_tr_labels_test = train_test_split(ros_tr_set,ros_tr_labels, test_size=0.2, random_state=42)\n\n#4. split renn_boruta_set and renn_boruta_labels\nrenn_boruta_set_train, renn_boruta_set_test, renn_boruta_labels_train, renn_boruta_labels_test = train_test_split(renn_boruta_set,renn_boruta_labels, test_size=0.2, random_state=42)\n\n#5. split renn_tr_set and renn_tr_labels\nrenn_tr_set_train, renn_tr_set_test, renn_tr_labels_train, renn_tr_labels_test = train_test_split(renn_tr_set,renn_tr_labels, test_size=0.2, random_state=42)","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#define the dateset list\ntrain_set_list = [original_set_train,ros_boruta_set_train, ros_tr_set_train,renn_boruta_set_train, renn_tr_set_train]\ntrain_labels_list = [original_labels_train,ros_boruta_labels_train, ros_tr_labels_train,renn_boruta_labels_train, renn_tr_labels_train]\ntest_set_list = [original_set_test,ros_boruta_set_test,ros_tr_set_test,renn_boruta_set_test,renn_tr_set_test]\ntest_labels_list = [original_labels_test,ros_boruta_labels_test,ros_tr_labels_test,renn_boruta_labels_test,renn_tr_labels_test]\ndataset_name_list = [\"original\",\"ros_boruta\",\"ros_tr\",\"renn_boruta\",\"renn_tr\"]","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Tree models\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.metrics import classification_report,confusion_matrix\n\ndtc = DecisionTreeClassifier()","execution_count":35,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(5):\n  #train the model against 5 dataset\n  dtc.fit(train_set_list[i],train_labels_list[i])\n  \n  dtc_predicted = dtc.predict(test_set_list[i])\n  #dtc_confusion = confusion_matrix(test_labels_list[i], dtc_predicted)\n\n  print(classification_report(test_labels_list[i], dtc_predicted))\n\n  print('Micro-averaged precision = {:.2f} (treat instances equally)'\n      .format(precision_score(test_labels_list[i], dtc_predicted, average = 'micro')))\n  print('Macro-averaged precision = {:.2f} (treat classes equally)'\n      .format(precision_score(test_labels_list[i], dtc_predicted, average = 'macro')))\n\n  print('Micro-averaged f1 = {:.2f} (treat instances equally)'\n        .format(f1_score(test_labels_list[i], dtc_predicted, average = 'micro')))\n  print('Macro-averaged f1 = {:.2f} (treat classes equally)'\n        .format(f1_score(test_labels_list[i], dtc_predicted, average = 'macro')))\n  print(\"--------------------------------------------------------------------------\")\n","execution_count":37,"outputs":[{"output_type":"stream","text":"                 precision    recall  f1-score   support\n\n            Low       0.83      0.86      0.84        28\n         Medium       0.83      0.80      0.81        44\nModerately High       0.67      0.73      0.70        11\n\n       accuracy                           0.81        83\n      macro avg       0.78      0.79      0.78        83\n   weighted avg       0.81      0.81      0.81        83\n\nMicro-averaged precision = 0.81 (treat instances equally)\nMacro-averaged precision = 0.78 (treat classes equally)\nMicro-averaged f1 = 0.81 (treat instances equally)\nMacro-averaged f1 = 0.78 (treat classes equally)\n--------------------------------------------------------------------------\n                 precision    recall  f1-score   support\n\n           High       0.92      1.00      0.96        47\n            Low       0.86      0.95      0.90        40\n         Medium       0.90      0.59      0.71        46\nModerately High       0.77      0.90      0.83        48\n\n       accuracy                           0.86       181\n      macro avg       0.86      0.86      0.85       181\n   weighted avg       0.86      0.86      0.85       181\n\nMicro-averaged precision = 0.86 (treat instances equally)\nMacro-averaged precision = 0.86 (treat classes equally)\nMicro-averaged f1 = 0.86 (treat instances equally)\nMacro-averaged f1 = 0.85 (treat classes equally)\n--------------------------------------------------------------------------\n                 precision    recall  f1-score   support\n\n           High       0.90      1.00      0.95        47\n            Low       0.84      0.95      0.89        40\n         Medium       0.90      0.57      0.69        46\nModerately High       0.78      0.90      0.83        48\n\n       accuracy                           0.85       181\n      macro avg       0.86      0.85      0.84       181\n   weighted avg       0.86      0.85      0.84       181\n\nMicro-averaged precision = 0.85 (treat instances equally)\nMacro-averaged precision = 0.86 (treat classes equally)\nMicro-averaged f1 = 0.85 (treat instances equally)\nMacro-averaged f1 = 0.84 (treat classes equally)\n--------------------------------------------------------------------------\n                 precision    recall  f1-score   support\n\n            Low       1.00      1.00      1.00        17\n         Medium       1.00      1.00      1.00        24\nModerately High       1.00      1.00      1.00         6\n\n       accuracy                           1.00        47\n      macro avg       1.00      1.00      1.00        47\n   weighted avg       1.00      1.00      1.00        47\n\nMicro-averaged precision = 1.00 (treat instances equally)\nMacro-averaged precision = 1.00 (treat classes equally)\nMicro-averaged f1 = 1.00 (treat instances equally)\nMacro-averaged f1 = 1.00 (treat classes equally)\n--------------------------------------------------------------------------\n                 precision    recall  f1-score   support\n\n           High       0.00      0.00      0.00         0\n            Low       1.00      1.00      1.00        13\n         Medium       1.00      0.92      0.96        26\nModerately High       1.00      1.00      1.00         5\n\n       accuracy                           0.95        44\n      macro avg       0.75      0.73      0.74        44\n   weighted avg       1.00      0.95      0.98        44\n\nMicro-averaged precision = 0.95 (treat instances equally)\nMacro-averaged precision = 0.75 (treat classes equally)\nMicro-averaged f1 = 0.95 (treat instances equally)\nMacro-averaged f1 = 0.74 (treat classes equally)\n--------------------------------------------------------------------------\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Linear Model\nfrom sklearn.svm import SVC\n\nfor i in range(5):\n  #train the model against 5 dataset\n  svm = SVC(kernel = 'linear').fit(train_set_list[i], train_labels_list[i])\n  svm_predicted = svm.predict(test_set_list[i])\n\n  print(classification_report(test_labels_list[i], svm_predicted))\n\n  print('Micro-averaged precision = {:.2f}'\n      .format(precision_score(test_labels_list[i], svm_predicted, average = 'micro')))\n  print('Macro-averaged precision = {:.2f}'\n      .format(precision_score(test_labels_list[i], svm_predicted, average = 'macro')))\n\n  print('Micro-averaged f1 = {:.2f}'\n        .format(f1_score(test_labels_list[i], svm_predicted, average = 'micro')))\n  print('Macro-averaged f1 = {:.2f} '\n        .format(f1_score(test_labels_list[i], svm_predicted, average = 'macro')))\n  print(\"--------------------------------------------------------------------------\")\n","execution_count":40,"outputs":[{"output_type":"stream","text":"                 precision    recall  f1-score   support\n\n            Low       0.88      0.79      0.83        28\n         Medium       0.71      0.93      0.80        44\nModerately High       0.00      0.00      0.00        11\n\n       accuracy                           0.76        83\n      macro avg       0.53      0.57      0.54        83\n   weighted avg       0.67      0.76      0.71        83\n\nMicro-averaged precision = 0.76\nMacro-averaged precision = 0.53\nMicro-averaged f1 = 0.76\nMacro-averaged f1 = 0.54 \n--------------------------------------------------------------------------\n                 precision    recall  f1-score   support\n\n           High       0.44      0.40      0.42        47\n            Low       0.91      0.80      0.85        40\n         Medium       0.40      0.04      0.08        46\nModerately High       0.48      0.98      0.64        48\n\n       accuracy                           0.55       181\n      macro avg       0.56      0.56      0.50       181\n   weighted avg       0.55      0.55      0.49       181\n\nMicro-averaged precision = 0.55\nMacro-averaged precision = 0.56\nMicro-averaged f1 = 0.55\nMacro-averaged f1 = 0.50 \n--------------------------------------------------------------------------\n                 precision    recall  f1-score   support\n\n           High       0.42      0.40      0.41        47\n            Low       0.88      0.72      0.79        40\n         Medium       0.20      0.02      0.04        46\nModerately High       0.48      0.98      0.64        48\n\n       accuracy                           0.53       181\n      macro avg       0.50      0.53      0.47       181\n   weighted avg       0.48      0.53      0.46       181\n\nMicro-averaged precision = 0.53\nMacro-averaged precision = 0.50\nMicro-averaged f1 = 0.53\nMacro-averaged f1 = 0.47 \n--------------------------------------------------------------------------\n                 precision    recall  f1-score   support\n\n            Low       1.00      0.94      0.97        17\n         Medium       0.77      1.00      0.87        24\nModerately High       0.00      0.00      0.00         6\n\n       accuracy                           0.85        47\n      macro avg       0.59      0.65      0.61        47\n   weighted avg       0.76      0.85      0.80        47\n\nMicro-averaged precision = 0.85\nMacro-averaged precision = 0.59\nMicro-averaged f1 = 0.85\nMacro-averaged f1 = 0.61 \n--------------------------------------------------------------------------\n                 precision    recall  f1-score   support\n\n            Low       1.00      0.77      0.87        13\n         Medium       0.76      1.00      0.87        26\nModerately High       0.00      0.00      0.00         5\n\n       accuracy                           0.82        44\n      macro avg       0.59      0.59      0.58        44\n   weighted avg       0.75      0.82      0.77        44\n\nMicro-averaged precision = 0.82\nMacro-averaged precision = 0.59\nMicro-averaged f1 = 0.82\nMacro-averaged f1 = 0.58 \n--------------------------------------------------------------------------\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Probabilistic models\nfrom sklearn.naive_bayes import GaussianNB\n\ngnb = GaussianNB()","execution_count":42,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(5):\n  #train the model against 5 dataset\n  gnb.fit(train_set_list[i],train_labels_list[i])\n  \n  gnb_predicted = gnb.predict(test_set_list[i])\n\n  print(classification_report(test_labels_list[i], gnb_predicted))\n\n  print('Micro-averaged precision = {:.2f}'\n      .format(precision_score(test_labels_list[i], gnb_predicted, average = 'micro')))\n  print('Macro-averaged precision = {:.2f}'\n      .format(precision_score(test_labels_list[i], gnb_predicted, average = 'macro')))\n\n  print('Micro-averaged f1 = {:.2f}'\n        .format(f1_score(test_labels_list[i], gnb_predicted, average = 'micro')))\n  print('Macro-averaged f1 = {:.2f}'\n        .format(f1_score(test_labels_list[i], gnb_predicted, average = 'macro')))\n  print(\"--------------------------------------------------------------------------\")\n","execution_count":43,"outputs":[{"output_type":"stream","text":"                 precision    recall  f1-score   support\n\n            Low       0.86      0.89      0.88        28\n         Medium       0.80      0.64      0.71        44\nModerately High       0.32      0.55      0.40        11\n\n       accuracy                           0.71        83\n      macro avg       0.66      0.69      0.66        83\n   weighted avg       0.76      0.71      0.72        83\n\nMicro-averaged precision = 0.71\nMacro-averaged precision = 0.66\nMicro-averaged f1 = 0.71\nMacro-averaged f1 = 0.66\n--------------------------------------------------------------------------\n                 precision    recall  f1-score   support\n\n           High       0.73      1.00      0.85        47\n            Low       0.92      0.82      0.87        40\n         Medium       0.71      0.26      0.38        46\nModerately High       0.64      0.85      0.73        48\n\n       accuracy                           0.73       181\n      macro avg       0.75      0.74      0.71       181\n   weighted avg       0.74      0.73      0.70       181\n\nMicro-averaged precision = 0.73\nMacro-averaged precision = 0.75\nMicro-averaged f1 = 0.73\nMacro-averaged f1 = 0.71\n--------------------------------------------------------------------------\n                 precision    recall  f1-score   support\n\n           High       0.62      0.66      0.64        47\n            Low       0.90      0.70      0.79        40\n         Medium       0.55      0.26      0.35        46\nModerately High       0.54      0.88      0.67        48\n\n       accuracy                           0.62       181\n      macro avg       0.65      0.62      0.61       181\n   weighted avg       0.64      0.62      0.61       181\n\nMicro-averaged precision = 0.62\nMacro-averaged precision = 0.65\nMicro-averaged f1 = 0.62\nMacro-averaged f1 = 0.61\n--------------------------------------------------------------------------\n                 precision    recall  f1-score   support\n\n            Low       1.00      0.94      0.97        17\n         Medium       0.95      0.88      0.91        24\nModerately High       0.67      1.00      0.80         6\n\n       accuracy                           0.91        47\n      macro avg       0.87      0.94      0.89        47\n   weighted avg       0.93      0.91      0.92        47\n\nMicro-averaged precision = 0.91\nMacro-averaged precision = 0.87\nMicro-averaged f1 = 0.91\nMacro-averaged f1 = 0.89\n--------------------------------------------------------------------------\n                 precision    recall  f1-score   support\n\n            Low       0.86      0.92      0.89        13\n         Medium       0.96      0.92      0.94        26\nModerately High       1.00      1.00      1.00         5\n\n       accuracy                           0.93        44\n      macro avg       0.94      0.95      0.94        44\n   weighted avg       0.93      0.93      0.93        44\n\nMicro-averaged precision = 0.93\nMacro-averaged precision = 0.94\nMicro-averaged f1 = 0.93\nMacro-averaged f1 = 0.94\n--------------------------------------------------------------------------\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Adaboost\nfrom sklearn.ensemble import AdaBoostClassifier\n\nadab = AdaBoostClassifier()\n\nfor i in range(5):\n  #train the model against 5 dataset\n  adab.fit(train_set_list[i],train_labels_list[i])\n  \n  adab_predicted = adab.predict(test_set_list[i])\n\n  print(classification_report(test_labels_list[i], adab_predicted))\n\n  print('Micro-averaged precision = {:.2f}'\n      .format(precision_score(test_labels_list[i], adab_predicted, average = 'micro')))\n  print('Macro-averaged precision = {:.2f}'\n      .format(precision_score(test_labels_list[i], adab_predicted, average = 'macro')))\n\n  print('Micro-averaged f1 = {:.2f}'\n        .format(f1_score(test_labels_list[i], adab_predicted, average = 'micro')))\n  print('Macro-averaged f1 = {:.2f}'\n        .format(f1_score(test_labels_list[i], adab_predicted, average = 'macro')))\n  print(\"--------------------------------------------------------------------------\")\n","execution_count":44,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","name":"stderr"},{"output_type":"stream","text":"                 precision    recall  f1-score   support\n\n           High       0.00      0.00      0.00         0\n            Low       0.86      0.86      0.86        28\n         Medium       0.81      0.89      0.85        44\nModerately High       0.80      0.36      0.50        11\n\n       accuracy                           0.81        83\n      macro avg       0.62      0.53      0.55        83\n   weighted avg       0.83      0.81      0.80        83\n\nMicro-averaged precision = 0.81\nMacro-averaged precision = 0.62\nMicro-averaged f1 = 0.81\nMacro-averaged f1 = 0.55\n--------------------------------------------------------------------------\n                 precision    recall  f1-score   support\n\n           High       0.86      0.13      0.22        47\n            Low       0.74      0.93      0.82        40\n         Medium       0.48      0.35      0.41        46\nModerately High       0.52      0.98      0.68        48\n\n       accuracy                           0.59       181\n      macro avg       0.65      0.59      0.53       181\n   weighted avg       0.65      0.59      0.52       181\n\nMicro-averaged precision = 0.59\nMacro-averaged precision = 0.65\nMicro-averaged f1 = 0.59\nMacro-averaged f1 = 0.53\n--------------------------------------------------------------------------\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","name":"stderr"},{"output_type":"stream","text":"                 precision    recall  f1-score   support\n\n           High       0.00      0.00      0.00        47\n            Low       0.84      0.65      0.73        40\n         Medium       0.44      0.65      0.53        46\nModerately High       0.52      0.90      0.66        48\n\n       accuracy                           0.55       181\n      macro avg       0.45      0.55      0.48       181\n   weighted avg       0.44      0.55      0.47       181\n\nMicro-averaged precision = 0.55\nMacro-averaged precision = 0.45\nMicro-averaged f1 = 0.55\nMacro-averaged f1 = 0.48\n--------------------------------------------------------------------------\n                 precision    recall  f1-score   support\n\n            Low       1.00      0.12      0.21        17\n         Medium       0.55      0.88      0.68        24\nModerately High       0.57      0.67      0.62         6\n\n       accuracy                           0.57        47\n      macro avg       0.71      0.55      0.50        47\n   weighted avg       0.72      0.57      0.50        47\n\nMicro-averaged precision = 0.57\nMacro-averaged precision = 0.71\nMicro-averaged f1 = 0.57\nMacro-averaged f1 = 0.50\n--------------------------------------------------------------------------\n                 precision    recall  f1-score   support\n\n           High       0.00      0.00      0.00         0\n            Low       1.00      0.15      0.27        13\n         Medium       0.63      0.92      0.75        26\nModerately High       1.00      0.40      0.57         5\n\n       accuracy                           0.64        44\n      macro avg       0.66      0.37      0.40        44\n   weighted avg       0.78      0.64      0.59        44\n\nMicro-averaged precision = 0.64\nMacro-averaged precision = 0.66\nMicro-averaged f1 = 0.64\nMacro-averaged f1 = 0.40\n--------------------------------------------------------------------------\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","name":"stderr"}]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}